{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDmHydgyHtRo"
      },
      "outputs": [],
      "source": [
        "!pip install nvcc4jupyter\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext nvcc4jupyter\n"
      ],
      "metadata": {
        "id": "bPcCqFHLINMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "code = \"\"\"#include <stdio.h>\n",
        "#include <cuda.h>\n",
        "#include <math.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "#define TM 4\n",
        "#define TN 2\n",
        "#define BLOCK_SIZE 16\n",
        "#define WARMUP_RUNS 3\n",
        "#define TIMING_RUNS 5\n",
        "\n",
        "// Using the tools I learned in MatMult, I decided to implement them in a forward and backward pass.\n",
        "// First, I implemented the naive version of forward and backward pass.\n",
        "// Then, I used tiling and vectorization to optimize them, resulting in ~2x speed up\n",
        "\n",
        "\n",
        "//Naive version of forward pass\n",
        "//I also added bias and a sigmoid activation function into the forward pass\n",
        "__global__ void forward_naive(const float* X, const float* W, float* Y,\n",
        "                              int num_samples, int input_dim, int num_features, const float* bias) {\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (row < num_samples && col < num_features) {\n",
        "        float sum = 0.0f;\n",
        "        for (int i = 0; i < input_dim; ++i) {\n",
        "            sum += X[row * input_dim + i] * W[i * num_features + col];\n",
        "        }\n",
        "        sum += bias[col];\n",
        "        sum = 1.0f / (1.0f + expf(-sum));\n",
        "        Y[row * num_features + col] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "// Optimized version of forward pass\n",
        "// First, I wanted to implement vectorization and tiling.\n",
        "__global__ void forward_vectorized(const float* X, const float* W, float* Y,\n",
        "                                   int num_samples, int input_dim, int num_features, const float* bias) {\n",
        "\n",
        "    // Shared memory for tiles\n",
        "    __shared__ float X_tile[BLOCK_SIZE][BLOCK_SIZE * 4];\n",
        "    __shared__ float W_tile[BLOCK_SIZE * 4][BLOCK_SIZE];\n",
        "\n",
        "    int threadRow = threadIdx.y;\n",
        "    int threadCol = threadIdx.x;\n",
        "\n",
        "    int globalRowStart = blockIdx.y * BLOCK_SIZE + threadRow * TM;\n",
        "    int globalColStart = blockIdx.x * BLOCK_SIZE + threadCol * TN;\n",
        "\n",
        "    //threads hold 4x2 results = 8\n",
        "    float threadResults[TM][TN] = {0};\n",
        "\n",
        "    int numTiles = (input_dim + BLOCK_SIZE * 4 - 1) / (BLOCK_SIZE * 4);\n",
        "\n",
        "    for (int tileIdx = 0; tileIdx < numTiles; tileIdx++) {\n",
        "        int tileColStart = tileIdx * BLOCK_SIZE * 4;\n",
        "        int tileRowStart = tileIdx * BLOCK_SIZE * 4;\n",
        "\n",
        "        // Load X_tile: Each thread loads multiple elements\n",
        "        // X_tile[BLOCK_SIZE][BLOCK_SIZE * 4] maps to X[num_samples][input_dim]\n",
        "        for (int m = 0; m < TM; m++) {\n",
        "            int globalRow = globalRowStart + m;\n",
        "\n",
        "            // Each thread loads 4 consecutive elements per iteration\n",
        "            for (int loadIdx = 0; loadIdx < (BLOCK_SIZE * 4) / (BLOCK_SIZE / TN); loadIdx += 4) {\n",
        "                int tileCol = threadCol * ((BLOCK_SIZE * 4) / (BLOCK_SIZE / TN)) + loadIdx;\n",
        "                int globalCol = tileColStart + tileCol;\n",
        "\n",
        "                if (globalRow < num_samples && globalCol + 3 < input_dim && tileCol + 3 < BLOCK_SIZE * 4) {\n",
        "                    // Vectorized load. Take 4 floats at a time if globalcol and titlecol are divisible by 4\n",
        "                    float4 vecX = *reinterpret_cast<const float4*>(&X[globalRow * input_dim + globalCol]);\n",
        "                    X_tile[threadRow * TM + m][tileCol + 0] = vecX.x;\n",
        "                    X_tile[threadRow * TM + m][tileCol + 1] = vecX.y;\n",
        "                    X_tile[threadRow * TM + m][tileCol + 2] = vecX.z;\n",
        "                    X_tile[threadRow * TM + m][tileCol + 3] = vecX.w;\n",
        "                } else {\n",
        "                    // If not divisible by 4\n",
        "                    for (int j = 0; j < 4 && tileCol + j < BLOCK_SIZE * 4; j++) {\n",
        "                        int col = globalCol + j;\n",
        "                        X_tile[threadRow * TM + m][tileCol + j] =\n",
        "                            (globalRow < num_samples && col < input_dim) ?\n",
        "                            X[globalRow * input_dim + col] : 0.0f;\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "\n",
        "        // Load W_tile: Each thread loads elements for its output columns\n",
        "        // W_tile can't be loaded in vectorized form since I go column-wise.\n",
        "        // I tried to implement row-wise by ended up with slower inference time.\n",
        "        for (int loadRow = 0; loadRow < (BLOCK_SIZE * 4) / (BLOCK_SIZE / TM); loadRow++) {\n",
        "            int tileRow = threadRow * ((BLOCK_SIZE * 4) / (BLOCK_SIZE / TM)) + loadRow;\n",
        "            int globalRow = tileRowStart + tileRow;\n",
        "\n",
        "            for (int n = 0; n < TN; n++) {\n",
        "                int globalCol = globalColStart + n;\n",
        "\n",
        "                if (globalRow < input_dim && globalCol < num_features && tileRow < BLOCK_SIZE * 4) {\n",
        "                    W_tile[tileRow][threadCol * TN + n] = W[globalRow * num_features + globalCol];\n",
        "                } else if (tileRow < BLOCK_SIZE * 4) {\n",
        "                    W_tile[tileRow][threadCol * TN + n] = 0.0f;\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "\n",
        "        __syncthreads();\n",
        "\n",
        "        // Compute partial dot product\n",
        "        for (int k = 0; k < BLOCK_SIZE * 4; k++) {\n",
        "            for (int m = 0; m < TM; m++) {\n",
        "                float x_val = X_tile[threadRow * TM + m][k];\n",
        "                for (int n = 0; n < TN; n++) {\n",
        "                    threadResults[m][n] += x_val * W_tile[k][threadCol * TN + n];\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // Write results back to global memory with bias and activation\n",
        "    for (int m = 0; m < TM; m++) {\n",
        "        for (int n = 0; n < TN; n++) {\n",
        "            int row = globalRowStart + m;\n",
        "            int col = globalColStart + n;\n",
        "\n",
        "            if (row < num_samples && col < num_features) {\n",
        "                float result = threadResults[m][n] + bias[col];  // Add bias\n",
        "                result = 1.0f / (1.0f + expf(-result));  // Sigmoid activation\n",
        "                Y[row * num_features + col] = result;\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "// Naive implementation of backward pass\n",
        "__global__ void backward_naive(\n",
        "    const float* X, const float* Y_pred, const float* Y_true,\n",
        "    float* dW,\n",
        "    int num_samples, int input_dim, int num_features)\n",
        "{\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;  // input dimension index\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;  // feature dimension index\n",
        "\n",
        "    if (row >= input_dim || col >= num_features) return;\n",
        "\n",
        "    float grad_w = 0.0f;\n",
        "\n",
        "    // Accumulate gradient across all samples\n",
        "    for (int s = 0; s < num_samples; ++s) {\n",
        "        float y_pred = Y_pred[s * num_features + col];\n",
        "        float y_true = Y_true[s * num_features + col];\n",
        "        // gradient of loss function!(y_pred - y_true)\n",
        "        // sigmoid derivative=(y_pred * (1.0f - y_pred);))\n",
        "        float dL_dZ = (y_pred - y_true) * y_pred * (1.0f - y_pred);\n",
        "\n",
        "        // Accumulate gradient: dL/dW = X^T * dL/dZ\n",
        "        grad_w += X[s * input_dim + row] * dL_dZ;\n",
        "    }\n",
        "    //full gradient matrix\n",
        "    dW[row * num_features + col] = grad_w;\n",
        "}\n",
        "\n",
        "\n",
        "// naive version of backward bias\n",
        "// gradient of bias is the sum of dl_dz.\n",
        "__global__ void backward_bias_naive(\n",
        "    const float* Y_pred, const float* Y_true,\n",
        "    float* db,\n",
        "    int num_samples, int num_features)\n",
        "{\n",
        "    int j = blockIdx.x * blockDim.x + threadIdx.x;  // feature dimension index\n",
        "\n",
        "    if (j >= num_features) return;\n",
        "\n",
        "    float grad_b = 0.0f;\n",
        "\n",
        "    // Accumulate gradient across all samples\n",
        "    for (int s = 0; s < num_samples; ++s) {\n",
        "        float y_pred = Y_pred[s * num_features + j];\n",
        "        float y_true = Y_true[s * num_features + j];\n",
        "\n",
        "        // Compute gradient of loss w.r.t. pre-activation (sigmoid derivative)\n",
        "        float dL_dZ = (y_pred - y_true) * y_pred * (1.0f - y_pred);\n",
        "\n",
        "        // Accumulate bias gradient: dL/db = sum(dL/dZ)\n",
        "        grad_b += dL_dZ;\n",
        "    }\n",
        "\n",
        "    db[j] = grad_b;\n",
        "}\n",
        "\n",
        "\n",
        "//\n",
        "__global__ void backward_optimized(\n",
        "    const float* X, const float* Y_pred, const float* Y_true,\n",
        "    float* dW,\n",
        "    int num_samples, int input_dim, int num_features)\n",
        "{\n",
        "    __shared__ float X_tile[BLOCK_SIZE * 4][BLOCK_SIZE];\n",
        "    __shared__ float dL_dZ_tile[BLOCK_SIZE][BLOCK_SIZE];\n",
        "\n",
        "    int threadRow = threadIdx.y;\n",
        "    int threadCol = threadIdx.x;\n",
        "\n",
        "    int globalRowStart = blockIdx.y * BLOCK_SIZE + threadRow * TM;\n",
        "    int globalColStart = blockIdx.x * BLOCK_SIZE + threadCol * TN;\n",
        "\n",
        "    float threadResults[TM][TN] = {0};\n",
        "\n",
        "    int numTiles = (num_samples + BLOCK_SIZE - 1) / BLOCK_SIZE;\n",
        "\n",
        "    #pragma unroll\n",
        "    for (int tileIdx = 0; tileIdx < numTiles; tileIdx++) {\n",
        "        int sampleTileStart = tileIdx * BLOCK_SIZE;\n",
        "\n",
        "        // Load X tile (transposed for this computation)\n",
        "        #pragma unroll\n",
        "        for (int i = 0; i < 4; i++) {\n",
        "            int inputRow = globalRowStart + i;\n",
        "            #pragma unroll\n",
        "            for (int n = 0; n < TN; n++) {\n",
        "                int sampleCol = sampleTileStart + threadCol * TN + n;\n",
        "\n",
        "                if (inputRow < input_dim && sampleCol < num_samples) {\n",
        "                    X_tile[threadRow * 4 + i][threadCol * TN + n] = X[sampleCol * input_dim + inputRow];\n",
        "                } else {\n",
        "                    X_tile[threadRow * 4 + i][threadCol * TN + n] = 0.0f;\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "\n",
        "        #pragma unroll\n",
        "        for (int m = 0; m < TM; m++) {\n",
        "            int sampleRow = sampleTileStart + threadRow * TM + m;\n",
        "            #pragma unroll\n",
        "            for (int n = 0; n < TN; n++) {\n",
        "                int featureCol = globalColStart + n;\n",
        "\n",
        "                if (sampleRow < num_samples && featureCol < num_features) {\n",
        "                    // Use precomputed forward pass results\n",
        "                    float y_pred = Y_pred[sampleRow * num_features + featureCol];\n",
        "                    float y_true = Y_true[sampleRow * num_features + featureCol];\n",
        "\n",
        "                    // Compute gradient of loss w.r.t. z\n",
        "                    dL_dZ_tile[threadRow * TM + m][threadCol * TN + n] =\n",
        "                        (y_pred - y_true) * y_pred * (1.0f - y_pred);\n",
        "                } else {\n",
        "                    dL_dZ_tile[threadRow * TM + m][threadCol * TN + n] = 0.0f;\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "\n",
        "        __syncthreads();\n",
        "\n",
        "        // Compute partial gradient: X^T * dL_dZ\n",
        "        #pragma unroll\n",
        "        for (int k = 0; k < BLOCK_SIZE; k++) {\n",
        "            // Cache dL_dZ values for this k\n",
        "            float dL_dZ_cache[TN];\n",
        "            #pragma unroll\n",
        "            for (int n = 0; n < TN; n++) {\n",
        "                dL_dZ_cache[n] = dL_dZ_tile[k][threadCol * TN + n];\n",
        "            }\n",
        "\n",
        "            #pragma unroll\n",
        "            for (int m = 0; m < TM; m++) {\n",
        "                float x_val = X_tile[threadRow * TM + m][k];\n",
        "                #pragma unroll\n",
        "                for (int n = 0; n < TN; n++) {\n",
        "                    threadResults[m][n] += x_val * dL_dZ_cache[n];\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // Write results back to global memory\n",
        "    #pragma unroll\n",
        "    for (int m = 0; m < TM; m++) {\n",
        "        #pragma unroll\n",
        "        for (int n = 0; n < TN; n++) {\n",
        "            int inputRow = globalRowStart + m;\n",
        "            int featureCol = globalColStart + n;\n",
        "\n",
        "            if (inputRow < input_dim && featureCol < num_features) {\n",
        "                dW[inputRow * num_features + featureCol] = threadResults[m][n];\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "__global__ void backward_bias_vectorized(\n",
        "    const float* Y_pred, const float* Y_true,\n",
        "    float* db,\n",
        "    int num_samples, int num_features)\n",
        "{\n",
        "    const int globalColStart = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (globalColStart * 4 >= num_features) return;\n",
        "\n",
        "    float threadResults[4] = {0.0f, 0.0f, 0.0f, 0.0f};\n",
        "\n",
        "    // Process 4 samples at a time\n",
        "    int s = 0;\n",
        "    for (; s + 3 < num_samples; s += 4) {\n",
        "        #pragma unroll\n",
        "        for (int sample_offset = 0; sample_offset < 4; sample_offset++) {\n",
        "            const int base_idx = (s + sample_offset) * num_features + globalColStart * 4;\n",
        "\n",
        "            if (globalColStart * 4 + 3 < num_features) {\n",
        "                // Vectorized load with proper alignment\n",
        "                const float4 vecPred = __ldg(reinterpret_cast<const float4*>(&Y_pred[base_idx]));\n",
        "                const float4 vecTrue = __ldg(reinterpret_cast<const float4*>(&Y_true[base_idx]));\n",
        "\n",
        "                threadResults[0] += (vecPred.x - vecTrue.x) * vecPred.x * (1.0f - vecPred.x);\n",
        "                threadResults[1] += (vecPred.y - vecTrue.y) * vecPred.y * (1.0f - vecPred.y);\n",
        "                threadResults[2] += (vecPred.z - vecTrue.z) * vecPred.z * (1.0f - vecPred.z);\n",
        "                threadResults[3] += (vecPred.w - vecTrue.w) * vecPred.w * (1.0f - vecPred.w);\n",
        "            } else {\n",
        "                // Handle boundary case\n",
        "                #pragma unroll\n",
        "                for (int i = 0; i < 4 && globalColStart * 4 + i < num_features; i++) {\n",
        "                    const float y_pred = __ldg(&Y_pred[base_idx + i]);\n",
        "                    const float y_true = __ldg(&Y_true[base_idx + i]);\n",
        "                    threadResults[i] += (y_pred - y_true) * y_pred * (1.0f - y_pred);\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Write back results\n",
        "    if (globalColStart * 4 + 3 < num_features) {\n",
        "        const float4 result_vec = {threadResults[0], threadResults[1], threadResults[2], threadResults[3]};\n",
        "        *reinterpret_cast<float4*>(&db[globalColStart * 4]) = result_vec;\n",
        "    } else {\n",
        "        #pragma unroll\n",
        "        for (int i = 0; i < 4 && globalColStart * 4 + i < num_features; i++) {\n",
        "            db[globalColStart * 4 + i] = threadResults[i];\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int num_samples = 2048, input_dim = 1024, num_features = 5120;\n",
        "\n",
        "    size_t size_X = num_samples * input_dim * sizeof(float);\n",
        "    size_t size_W = input_dim * num_features * sizeof(float);\n",
        "    size_t size_Y = num_samples * num_features * sizeof(float);\n",
        "    size_t size_B = num_features * sizeof(float);\n",
        "\n",
        "    // Host memory allocation\n",
        "    float *h_X = (float*)malloc(size_X);\n",
        "    float *h_W = (float*)malloc(size_W);\n",
        "    float *h_Y_true = (float*)malloc(size_Y);\n",
        "    float *h_Y_pred_naive = (float*)malloc(size_Y);\n",
        "    float *h_Y_pred_vectorized = (float*)malloc(size_Y);\n",
        "    float *h_B = (float*)malloc(size_B);\n",
        "    float *h_dW_naive = (float*)malloc(size_W);\n",
        "    float *h_db_naive = (float*)malloc(size_B);\n",
        "    float *h_dW_vectorized = (float*)malloc(size_W);\n",
        "    float *h_db_vectorized = (float*)malloc(size_B);\n",
        "\n",
        "    // Initialize data with random values\n",
        "    srand(42);\n",
        "    for (int i = 0; i < num_samples * input_dim; i++) {\n",
        "        h_X[i] = ((float)rand() / RAND_MAX) * 2.0f - 1.0f;\n",
        "    }\n",
        "    for (int i = 0; i < input_dim * num_features; i++) {\n",
        "        h_W[i] = ((float)rand() / RAND_MAX) * 0.2f - 0.1f;\n",
        "    }\n",
        "    for (int i = 0; i < num_features; i++) {\n",
        "        h_B[i] = ((float)rand() / RAND_MAX) * 0.2f - 0.1f;\n",
        "    }\n",
        "    for (int i = 0; i < num_samples * num_features; i++) {\n",
        "        h_Y_true[i] = (float)rand() / RAND_MAX;\n",
        "    }\n",
        "\n",
        "    // Device memory allocation\n",
        "    float *d_X, *d_W, *d_Y, *d_B, *d_Y_true, *d_dW, *d_db;\n",
        "    cudaMalloc(&d_X, size_X);\n",
        "    cudaMalloc(&d_W, size_W);\n",
        "    cudaMalloc(&d_Y, size_Y);\n",
        "    cudaMalloc(&d_B, size_B);\n",
        "    cudaMalloc(&d_Y_true, size_Y);\n",
        "    cudaMalloc(&d_dW, size_W);\n",
        "    cudaMalloc(&d_db, size_B);\n",
        "\n",
        "    // Copy data to device\n",
        "    cudaMemcpy(d_X, h_X, size_X, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_W, h_W, size_W, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_B, h_B, size_B, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_Y_true, h_Y_true, size_Y, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Grid and block dimensions\n",
        "    dim3 threads(BLOCK_SIZE, BLOCK_SIZE);\n",
        "    dim3 blocks_forward((num_features + BLOCK_SIZE - 1) / BLOCK_SIZE,\n",
        "                       (num_samples + BLOCK_SIZE - 1) / BLOCK_SIZE);\n",
        "    dim3 blocks_backward((num_features + BLOCK_SIZE - 1) / BLOCK_SIZE,\n",
        "                        (input_dim + BLOCK_SIZE - 1) / BLOCK_SIZE);\n",
        "    dim3 blocks_bias((num_features + BLOCK_SIZE - 1) / BLOCK_SIZE, 1);\n",
        "    dim3 threads_bias(BLOCK_SIZE, 1);\n",
        "\n",
        "    // Vectorized kernel dimensions\n",
        "    dim3 threadsPerBlock_vec(BLOCK_SIZE / TN, BLOCK_SIZE / TM);\n",
        "    dim3 blocksPerGrid_vec((num_features + BLOCK_SIZE - 1) / BLOCK_SIZE,\n",
        "                          (num_samples + BLOCK_SIZE - 1) / BLOCK_SIZE);\n",
        "\n",
        "    dim3 threadsPerBlock_back(BLOCK_SIZE / TN, BLOCK_SIZE / TM);\n",
        "    dim3 blocksPerGrid_back_weights((num_features + BLOCK_SIZE - 1) / BLOCK_SIZE,\n",
        "                                   (input_dim + BLOCK_SIZE - 1) / BLOCK_SIZE);\n",
        "    dim3 blocksPerGrid_back_bias((num_features + BLOCK_SIZE - 1) / BLOCK_SIZE, 1);\n",
        "    dim3 threadsPerBlock_bias(BLOCK_SIZE / TN, 1);\n",
        "\n",
        "    // CUDA events for timing\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "\n",
        "    printf(\"=== WARMUP PHASE ===\");\n",
        "    // WARMUP RUNS - Don't measure these\n",
        "    for (int run = 0; run < WARMUP_RUNS; ++run) {\n",
        "        forward_naive<<<blocks_forward, threads>>>(d_X, d_W, d_Y, num_samples, input_dim, num_features, d_B);\n",
        "        forward_vectorized<<<blocksPerGrid_vec, threadsPerBlock_vec>>>(d_X, d_W, d_Y, num_samples, input_dim, num_features, d_B);\n",
        "        backward_naive<<<blocks_backward, threads>>>(d_X, d_Y, d_Y_true, d_dW,\n",
        "                                                      num_samples, input_dim, num_features);\n",
        "        backward_bias_naive<<<blocks_bias, threads_bias>>>(d_Y, d_Y_true, d_db,\n",
        "                                                           num_samples, num_features);\n",
        "        // Updated vectorized backward calls with Y_pred parameter\n",
        "        backward_optimized<<<blocksPerGrid_back_weights, threadsPerBlock_back>>>(d_X, d_Y, d_Y_true, d_dW,\n",
        "                                                                                 num_samples, input_dim, num_features);\n",
        "        backward_bias_vectorized<<<blocksPerGrid_back_bias, threadsPerBlock_bias>>>(d_Y, d_Y_true, d_db,\n",
        "                                                                                  num_samples, num_features);\n",
        "        cudaDeviceSynchronize(); // Make sure all kernels complete\n",
        "    }\n",
        "    printf(\"Warmup completed (%d runs)\", WARMUP_RUNS);\n",
        "\n",
        "    printf(\"=== TIMING PHASE ===\");\n",
        "\n",
        "    // TIMED FORWARD PASS - Naive version\n",
        "    float total_forward_naive = 0.0f;\n",
        "    for (int run = 0; run < TIMING_RUNS; ++run) {\n",
        "        cudaEventRecord(start);\n",
        "        forward_naive<<<blocks_forward, threads>>>(d_X, d_W, d_Y, num_samples, input_dim, num_features, d_B);\n",
        "        cudaEventRecord(stop);\n",
        "        cudaEventSynchronize(stop);\n",
        "        float ms;\n",
        "        cudaEventElapsedTime(&ms, start, stop);\n",
        "        total_forward_naive += ms;\n",
        "    }\n",
        "    // Copy naive forward results back to host\n",
        "    cudaMemcpy(h_Y_pred_naive, d_Y, size_Y, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // TIMED FORWARD PASS - Vectorized version\n",
        "    float total_forward_vectorized = 0.0f;\n",
        "    for (int run = 0; run < TIMING_RUNS; ++run) {\n",
        "        cudaEventRecord(start);\n",
        "        forward_vectorized<<<blocksPerGrid_vec, threadsPerBlock_vec>>>(d_X, d_W, d_Y, num_samples, input_dim, num_features, d_B);\n",
        "        cudaEventRecord(stop);\n",
        "        cudaEventSynchronize(stop);\n",
        "        float ms;\n",
        "        cudaEventElapsedTime(&ms, start, stop);\n",
        "        total_forward_vectorized += ms;\n",
        "    }\n",
        "    // Copy vectorized forward results back to host\n",
        "    cudaMemcpy(h_Y_pred_vectorized, d_Y, size_Y, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // TIMED BACKWARD PASS (using naive version)\n",
        "    float total_backward = 0.0f;\n",
        "    for (int run = 0; run < TIMING_RUNS; ++run) {\n",
        "\n",
        "        cudaEventRecord(start);\n",
        "        backward_naive<<<blocks_backward, threads>>>(d_X, d_Y, d_Y_true, d_dW,\n",
        "                                                      num_samples, input_dim, num_features);\n",
        "        backward_bias_naive<<<blocks_bias, threads_bias>>>(d_Y, d_Y_true, d_db,\n",
        "                                                           num_samples, num_features);\n",
        "        cudaEventRecord(stop);\n",
        "        cudaEventSynchronize(stop);\n",
        "        float ms;\n",
        "        cudaEventElapsedTime(&ms, start, stop);\n",
        "        total_backward += ms;\n",
        "    }\n",
        "\n",
        "    // Copy backward results back to host\n",
        "    cudaMemcpy(h_dW_naive, d_dW, size_W, cudaMemcpyDeviceToHost);\n",
        "    cudaMemcpy(h_db_naive, d_db, size_B, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // TIMED BACKWARD PASS (using vectorized version with precomputed Y_pred)\n",
        "    float total_backward_optimized = 0.0f;\n",
        "    for (int run = 0; run < TIMING_RUNS; ++run) {\n",
        "        cudaDeviceSynchronize();\n",
        "        cudaEventRecord(start);\n",
        "        // Use the precomputed Y_pred from the last forward pass\n",
        "        backward_optimized<<<blocksPerGrid_back_weights, threadsPerBlock_back>>>(\n",
        "            d_X, d_Y, d_Y_true, d_dW, num_samples, input_dim, num_features);\n",
        "        backward_bias_vectorized<<<blocksPerGrid_back_bias, threadsPerBlock_bias>>>(\n",
        "            d_Y, d_Y_true, d_db, num_samples, num_features);\n",
        "        cudaDeviceSynchronize();\n",
        "        cudaEventRecord(stop);\n",
        "        cudaEventSynchronize(stop);\n",
        "        float ms;\n",
        "        cudaEventElapsedTime(&ms, start, stop);\n",
        "        total_backward_optimized += ms;\n",
        "    }\n",
        "\n",
        "    cudaMemcpy(h_dW_vectorized, d_dW, size_W, cudaMemcpyDeviceToHost);\n",
        "    cudaMemcpy(h_db_vectorized, d_db, size_B, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // VERIFICATION: Compare forward pass results\n",
        "    printf(\"=== FORWARD PASS VERIFICATION ===\");\n",
        "    float max_forward_diff = 0.0f;\n",
        "    float avg_forward_diff = 0.0f;\n",
        "    for (int i = 0; i < num_samples * num_features; i++) {\n",
        "        float diff = fabsf(h_Y_pred_naive[i] - h_Y_pred_vectorized[i]);\n",
        "        if (diff > max_forward_diff) max_forward_diff = diff;\n",
        "        avg_forward_diff += diff;\n",
        "    }\n",
        "    avg_forward_diff /= (num_samples * num_features);\n",
        "    printf(\"Forward pass - Max difference: %.2e, Avg difference: %.2e\", max_forward_diff, avg_forward_diff);\n",
        "\n",
        "    // VERIFICATION: Compare backward pass results\n",
        "    printf(\"=== BACKWARD PASS VERIFICATION ===\");\n",
        "    float max_weight_diff = 0.0f;\n",
        "    float avg_weight_diff = 0.0f;\n",
        "    for (int i = 0; i < input_dim * num_features; i++) {\n",
        "        float diff = fabsf(h_dW_naive[i] - h_dW_vectorized[i]);\n",
        "        if (diff > max_weight_diff) max_weight_diff = diff;\n",
        "        avg_weight_diff += diff;\n",
        "    }\n",
        "    avg_weight_diff /= (input_dim * num_features);\n",
        "    printf(\"Weight gradients - Max difference: %.2e, Avg difference: %.2e\", max_weight_diff, avg_weight_diff);\n",
        "\n",
        "    float max_bias_diff = 0.0f;\n",
        "    float avg_bias_diff = 0.0f;\n",
        "    for (int i = 0; i < num_features; i++) {\n",
        "        float diff = fabsf(h_db_naive[i] - h_db_vectorized[i]);\n",
        "        if (diff > max_bias_diff) max_bias_diff = diff;\n",
        "        avg_bias_diff += diff;\n",
        "    }\n",
        "    avg_bias_diff /= num_features;\n",
        "    printf(\"Bias gradients - Max difference: %.2e, Avg difference: %.2e\", max_bias_diff, avg_bias_diff);\n",
        "\n",
        "    // Print results\n",
        "    printf(\"=== FORWARD PASS SAMPLE ===\");\n",
        "    printf(\"First 5 predictions (Naive vs Vectorized):\");\n",
        "    for (int i = 0; i < 5; ++i) {\n",
        "        printf(\"Y_pred[%d]: Naive=%.6f, Vectorized=%.6f, Y_true=%.6f\",\n",
        "               i, h_Y_pred_naive[i], h_Y_pred_vectorized[i], h_Y_true[i]);\n",
        "    }\n",
        "\n",
        "    printf(\"=== BACKWARD GRADIENT SAMPLES ==\");\n",
        "    printf(\"First 5 weight gradients (Naive vs Vectorized):\");\n",
        "    for (int i = 0; i < 5; ++i) {\n",
        "        printf(\"dW[%d]: Naive=%.6f, Vectorized=%.6f\", i, h_dW_naive[i], h_dW_vectorized[i]);\n",
        "    }\n",
        "    printf(\"First 5 bias gradients (Naive vs Vectorized):\");\n",
        "    for (int i = 0; i < 5; ++i) {\n",
        "        printf(\"db[%d]: Naive=%.6f, Vectorized=%.6f\", i, h_db_naive[i], h_db_vectorized[i]);\n",
        "    }\n",
        "\n",
        "    printf(\"=== PERFORMANCE RESULTS ===\");\n",
        "    printf(\"Forward pass (naive) avg time:      %.3f ms (over %d runs)\",\n",
        "           total_forward_naive / TIMING_RUNS, TIMING_RUNS);\n",
        "    printf(\"Forward pass (vectorized) avg time: %.3f ms (over %d runs)\",\n",
        "           total_forward_vectorized / TIMING_RUNS, TIMING_RUNS);\n",
        "    printf(\"Forward speedup (naive -> vectorized): %.2fx\",\n",
        "           total_forward_naive / total_forward_vectorized);\n",
        "    printf(\"Backward pass naive avg time:       %.3f ms (over %d runs)\",\n",
        "           total_backward / TIMING_RUNS, TIMING_RUNS);\n",
        "    printf(\"Backward pass vectorized avg time:  %.3f ms (over %d runs)\",\n",
        "           total_backward_optimized / TIMING_RUNS, TIMING_RUNS);\n",
        "    printf(\"Backward speedup (naive -> vectorized): %.2fx\",\n",
        "           total_backward / total_backward_optimized);\n",
        "    printf(\"total speedup (naive -> vectorized): %.2fx\",\n",
        "           (total_backward+total_forward_naive) / (total_forward_vectorized+total_backward_optimized));\n",
        "\n",
        "\n",
        "    // Compute simple loss for verification (using naive results)\n",
        "    float loss = 0.0f;\n",
        "    for (int i = 0; i < num_samples * num_features; ++i) {\n",
        "        float diff = h_Y_pred_naive[i] - h_Y_true[i];\n",
        "        loss += diff * diff;\n",
        "    }\n",
        "    loss /= (num_samples * num_features);\n",
        "    printf(\"Mean Squared Error:                 %.6f\", loss);\n",
        "\n",
        "    // Cleanup\n",
        "    cudaFree(d_X); cudaFree(d_W); cudaFree(d_Y); cudaFree(d_B);\n",
        "    cudaFree(d_Y_true); cudaFree(d_dW); cudaFree(d_db);\n",
        "    free(h_X); free(h_W); free(h_Y_pred_naive); free(h_Y_pred_vectorized);\n",
        "    free(h_Y_true); free(h_B); free(h_dW_naive); free(h_db_naive);\n",
        "    free(h_dW_vectorized); free(h_db_vectorized);\n",
        "    cudaEventDestroy(start); cudaEventDestroy(stop);\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "q6OvFkiBINSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"OptFCNN.cu\", \"w\") as f:\n",
        "    f.write(code)"
      ],
      "metadata": {
        "id": "69pwMlBtISD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_75 OptFCNN.cu -lcublas -o OptFCNN\n",
        "!./OptFCNN"
      ],
      "metadata": {
        "id": "mlEOhInuIS5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pbSHJBlfANkL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}