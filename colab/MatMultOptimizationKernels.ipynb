{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHSL-VZG9bE-"
      },
      "outputs": [],
      "source": [
        "!pip install nvcc4jupyter\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext nvcc4jupyter\n"
      ],
      "metadata": {
        "id": "RsWAWJYS9foE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "code = \"\"\"#include <stdio.h>\n",
        "#include <cuda.h>\n",
        "#include <math.h>\n",
        "#include <time.h>\n",
        "#include <cublas_v2.h>\n",
        "\n",
        "#define BLOCK_SIZE 16\n",
        "#define RUNS 5\n",
        "#define TM 4\n",
        "#define TN 2\n",
        "//Results are from using google colab T4 GPU. I also wrote in jupyter notebook if you want to run yourself on google colab\n",
        "// int num_samples = 2048, input_dim = 1024, num_features = 5120;\n",
        "\n",
        "// Kernel 1: This is basic/naive implementation of matrix multiplication in CUDA.\n",
        "// We uses a single thread to perform multiplication row-wise in X and column-wise in W.\n",
        "// Then accumulate the sum and write to a cell in the resulting matrix\n",
        "// Result: 46ms\n",
        "// Notes: going column-wise on W is not ideal as accessing the next row in W requires hopping num_samples memory blocks.\n",
        "//        This can be seen in W[i * num_features + col] where i is incremented, resulting in access to W to be inefficient.\n",
        "__global__ void matmul_kernel(const float* X, const float* W, float* Y,\n",
        "                              int num_samples, int input_dim, int num_features) {\n",
        "    //blockIdx = block index in grid\n",
        "    // blockDim = total num threads in block\n",
        "    // threadidx = thread index in block\n",
        "    //ex: to get thread 54 in y dimension, block idx.y = 3, block dim = 16, threadidx = 6.\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (row < num_samples && col < num_features) {\n",
        "        float sum = 0.0f;\n",
        "        for (int i = 0; i < input_dim; ++i) {\n",
        "            sum += X[row * input_dim + i] * W[i * num_features + col];\n",
        "        }\n",
        "        Y[row * num_features + col] = sum;\n",
        "    }\n",
        "}\n",
        "// Kernel 2: Accessing W in row-wise fashion\n",
        "// To make access to W more efficient, I transposed W first in a seperate kernel\n",
        "// Now, I can access both X and W in a row-wise fashion when doing matrix multiplication.\n",
        "// Result: 191ms\n",
        "// !!! The overhead of transposing W was costly and actually inefficient. a logical attempt but not worth in this instance.\n",
        "\n",
        "__global__ void transpose_kernel(const float* W, float* W_t, int input_dim, int num_features) {\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (row < input_dim && col < num_features) {\n",
        "        W_t[col * input_dim + row] = W[row * num_features + col];\n",
        "    }\n",
        "}\n",
        "__global__ void matmul_kernel_optimized(const float* X, const float* W_t, float* Y,\n",
        "                                        int num_samples, int input_dim, int num_features) {\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (row < num_samples && col < num_features) {\n",
        "        float sum = 0.0f;\n",
        "        for (int i = 0; i < input_dim; ++i) {\n",
        "            sum += X[row * input_dim + i] * W_t[col * input_dim + i];\n",
        "        }\n",
        "        Y[row * num_features + col] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "// Kernel 3:\n",
        "// Now, I'm implementing shared memory/tiling. Previously, we're accessing all matrices in the global memory space.\n",
        "// To reduce the number of times I access global memory, we take chunks or \"tiles\" of the matrices to shared memory, which is much faster access.\n",
        "// Tile sizes in this case are 16x16. num of Threads remain the same!\n",
        "// Each thread is still responsible for a single matmult in resulting matrix\n",
        "// Each thread holds the partial sum in the tiles.\n",
        "// Result: 28ms\n",
        "__global__ void matmul_kernel_shared(const float* X, const float* W, float* Y,\n",
        "                                     int num_samples, int input_dim, int num_features) {\n",
        "    // Shared memory for tiles\n",
        "    __shared__ float X_tile[BLOCK_SIZE][BLOCK_SIZE];\n",
        "    __shared__ float W_tile[BLOCK_SIZE][BLOCK_SIZE];\n",
        "\n",
        "    // Thread and block indices\n",
        "    int threadRow = threadIdx.y;\n",
        "    int threadCol = threadIdx.x;\n",
        "    int blockRow = blockIdx.y;\n",
        "    int blockCol = blockIdx.x;\n",
        "\n",
        "    // Global indices for this thread\n",
        "    int globalRow = blockRow * BLOCK_SIZE + threadRow;\n",
        "    int globalCol = blockCol * BLOCK_SIZE + threadCol;\n",
        "\n",
        "    // EACH SUM IS LOCAL TO THE THREAD\n",
        "    float sum = 0.0f;\n",
        "\n",
        "    // Loop over tiles along the K dimension (input_dim)\n",
        "    int numTiles = (input_dim + BLOCK_SIZE - 1) / BLOCK_SIZE;\n",
        "\n",
        "    // we iterate over all tiles. Threads are operating in parallel for every tile index INSIDE the for loop\n",
        "    // We use same threads per iteration\n",
        "    //the SAME thread iterates over tileidx. extracting 16x16 from global memory. and adding it to running sum per thread\n",
        "    // In this case, there are 256 threads within this block to load the 16x16 tiles.\n",
        "\n",
        "    // we also run blocks in parallel. so each block has 256 UNIQUE threads.\n",
        "    for (int tileIdx = 0; tileIdx < numTiles; tileIdx++) {\n",
        "        // Calculate tile starting positions\n",
        "        int X_tileCol = tileIdx * BLOCK_SIZE + threadCol;\n",
        "        int W_tileRow = tileIdx * BLOCK_SIZE + threadRow;\n",
        "\n",
        "\n",
        "        // Load X tile into shared memory\n",
        "        // Each thread loads one element: X[globalRow][X_tileCol]\n",
        "        // There are 256 threads. This loads the X_tile as 16x16\n",
        "        //\n",
        "        if (globalRow < num_samples && X_tileCol < input_dim) {\n",
        "            X_tile[threadRow][threadCol] = X[globalRow * input_dim + X_tileCol];\n",
        "        } else {\n",
        "            X_tile[threadRow][threadCol] = 0.0f;\n",
        "        }\n",
        "\n",
        "        // Load W tile into shared memory\n",
        "        // Each thread loads one element: W[W_tileRow][globalCol]\n",
        "        if (W_tileRow < input_dim && globalCol < num_features) {\n",
        "            W_tile[threadRow][threadCol] = W[W_tileRow * num_features + globalCol];\n",
        "        } else {\n",
        "            W_tile[threadRow][threadCol] = 0.0f;\n",
        "        }\n",
        "\n",
        "        // Synchronize to make sure tiles are loaded\n",
        "        __syncthreads();\n",
        "\n",
        "        //X_tile and W_tile get overwritten.\n",
        "\n",
        "        // Running Sum! The sum's are saved individually PER THREAD.\n",
        "        for (int k = 0; k < BLOCK_SIZE; k++) {\n",
        "            sum += X_tile[threadRow][k] * W_tile[k][threadCol];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // Write result to global memory\n",
        "    if (globalRow < num_samples && globalCol < num_features) {\n",
        "        Y[globalRow * num_features + globalCol] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "//Kernel 4: Increasing arithmatic indentensity by reducing number of threads being used.\n",
        "// To further optimize, we reduce have threads compute several results in resulting matrix.TM and TN.\n",
        "// This is stored in threadresults.\n",
        "// also reduced by caching rows of W .\n",
        "// Result: 23ms\n",
        "__global__ void matmul_kernel_threadreduce(const float* X, const float* W, float* Y,\n",
        "                                        int num_samples, int input_dim, int num_features) {\n",
        "\n",
        "    // Shared memory for tiles\n",
        "    __shared__ float X_tile[BLOCK_SIZE][BLOCK_SIZE];\n",
        "    __shared__ float W_tile[BLOCK_SIZE][BLOCK_SIZE];\n",
        "\n",
        "    // Thread and block indices\n",
        "    int threadRow = threadIdx.y;\n",
        "    int threadCol = threadIdx.x;\n",
        "    int blockRow = blockIdx.y;\n",
        "    int blockCol = blockIdx.x;\n",
        "\n",
        "    // With thread tiling, we need fewer threads per block\n",
        "    // Block dimensions are now (BLOCK_SIZE/TN, BLOCK_SIZE/TM)\n",
        "    // Each thread handles TM x TN output elements\n",
        "\n",
        "    // Starting global positions for this thread's output region\n",
        "    int globalRowStart = blockRow * BLOCK_SIZE + threadRow * TM;\n",
        "    int globalColStart = blockCol * BLOCK_SIZE + threadCol * TN;\n",
        "\n",
        "    // Thread-local register cache for results\n",
        "    // Each thread accumulates TM x TN results\n",
        "    float threadResults[TM][TN];\n",
        "\n",
        "    // Initialize results to zero\n",
        "    for (int m = 0; m < TM; m++) {\n",
        "        for (int n = 0; n < TN; n++) {\n",
        "            threadResults[m][n] = 0.0f;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Loop over tiles along the K dimension (input_dim)\n",
        "    int numTiles = (input_dim + BLOCK_SIZE - 1) / BLOCK_SIZE;\n",
        "\n",
        "    for (int tileIdx = 0; tileIdx < numTiles; tileIdx++) {\n",
        "        // Calculate tile starting positions\n",
        "        int X_tileCol = tileIdx * BLOCK_SIZE;\n",
        "        int W_tileRow = tileIdx * BLOCK_SIZE;\n",
        "\n",
        "        // Load X tile into shared memory\n",
        "        // Each thread loads multiple elements due to fewer threads\n",
        "        for (int m = 0; m < TM; m++) {\n",
        "            for (int k = 0; k < TN; k++) {\n",
        "                int loadRow = globalRowStart + m;\n",
        "                int loadCol = X_tileCol + threadCol * TN + k;\n",
        "                int smemRow = threadRow * TM + m;\n",
        "                int smemCol = threadCol * TN + k;\n",
        "\n",
        "                if (loadRow < num_samples && loadCol < input_dim &&\n",
        "                    smemRow < BLOCK_SIZE && smemCol < BLOCK_SIZE) {\n",
        "                    X_tile[smemRow][smemCol] = X[loadRow * input_dim + loadCol];\n",
        "                } else if (smemRow < BLOCK_SIZE && smemCol < BLOCK_SIZE) {\n",
        "                    X_tile[smemRow][smemCol] = 0.0f;\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "\n",
        "        // Load W tile into shared memory\n",
        "        // Each thread loads multiple elements due to fewer threads\n",
        "        for (int k = 0; k < TM; k++) {\n",
        "            for (int n = 0; n < TN; n++) {\n",
        "                int loadRow = W_tileRow + threadRow * TM + k;\n",
        "                int loadCol = globalColStart + n;\n",
        "                int smemRow = threadRow * TM + k;\n",
        "                int smemCol = threadCol * TN + n;\n",
        "\n",
        "                if (loadRow < input_dim && loadCol < num_features &&\n",
        "                    smemRow < BLOCK_SIZE && smemCol < BLOCK_SIZE) {\n",
        "                    W_tile[smemRow][smemCol] = W[loadRow * num_features + loadCol];\n",
        "                } else if (smemRow < BLOCK_SIZE && smemCol < BLOCK_SIZE) {\n",
        "                    W_tile[smemRow][smemCol] = 0.0f;\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "\n",
        "        __syncthreads();\n",
        "\n",
        "        // Compute partial results for this tile\n",
        "        // Each thread computes TM x TN partial dot products\n",
        "        for (int k = 0; k < BLOCK_SIZE; k++) {\n",
        "            // Cache W values for reuse across TM iterations\n",
        "            float W_cache[TN];\n",
        "            for (int n = 0; n < TN; n++) {\n",
        "                W_cache[n] = W_tile[k][threadCol * TN + n];\n",
        "            }\n",
        "\n",
        "            // Compute TM x TN partial products\n",
        "            for (int m = 0; m < TM; m++) {\n",
        "                float X_val = X_tile[threadRow * TM + m][k];\n",
        "                for (int n = 0; n < TN; n++) {\n",
        "                    threadResults[m][n] += X_val * W_cache[n];\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // Write results to global memory\n",
        "    // Each thread writes TM x TN output elements\n",
        "    for (int m = 0; m < TM; m++) {\n",
        "        for (int n = 0; n < TN; n++) {\n",
        "            int outputRow = globalRowStart + m;\n",
        "            int outputCol = globalColStart + n;\n",
        "\n",
        "            if (outputRow < num_samples && outputCol < num_features) {\n",
        "                Y[outputRow * num_features + outputCol] = threadResults[m][n];\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "// Kernel 5:\n",
        "// Building on top of thread reduction, now we implement vectorization\n",
        "// Everything is the same except previously, we loaded from global memory to shared memory(tile) at 1 float per iteration.\n",
        "// Since the locations of floats are next to each other, we can use float4 to get 4 floats at a time PER global mem access\n",
        "// This allows for less global memorry access.\n",
        "// The only thing that changed: Tile size is increased and XTile is loaded vectorized. Wtile is still loaded same way\n",
        "// I also implemented #pragma unrolling for slightly more efficient access during for loops\n",
        "// Result: 13ms\n",
        "\n",
        "__global__ void matmul_vectorized(const float* X, const float* W, float* Y,\n",
        "                                   int num_samples, int input_dim, int num_features) {\n",
        "\n",
        "    __shared__ float X_tile[BLOCK_SIZE][BLOCK_SIZE * 4 + 1];\n",
        "    __shared__ float W_tile[BLOCK_SIZE * 4][BLOCK_SIZE + 1];\n",
        "\n",
        "    const int threadRow = threadIdx.y;\n",
        "    const int threadCol = threadIdx.x;\n",
        "\n",
        "    const int globalRowStart = blockIdx.y * BLOCK_SIZE + threadRow * TM;\n",
        "    const int globalColStart = blockIdx.x * BLOCK_SIZE + threadCol * TN;\n",
        "\n",
        "    float threadResults[TM][TN];\n",
        "\n",
        "    #pragma unroll\n",
        "    for (int m = 0; m < TM; m++) {\n",
        "        #pragma unroll\n",
        "        for (int n = 0; n < TN; n++) {\n",
        "            threadResults[m][n] = 0.0f;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    const int numTiles = (input_dim + BLOCK_SIZE * 4 - 1) / (BLOCK_SIZE * 4);\n",
        "\n",
        "    for (int tileIdx = 0; tileIdx < numTiles; tileIdx++) {\n",
        "        const int X_tileCol = tileIdx * BLOCK_SIZE * 4;\n",
        "        const int W_tileRow = tileIdx * BLOCK_SIZE * 4;\n",
        "\n",
        "        // Load X_tile with your optimized approach\n",
        "        #pragma unroll\n",
        "        for (int m = 0; m < TM; m++) {\n",
        "            const int globalRow = globalRowStart + m;\n",
        "            const int colsPerThread = (BLOCK_SIZE * 4) / (BLOCK_SIZE / TN);\n",
        "\n",
        "            #pragma unroll\n",
        "            for (int i = 0; i < colsPerThread; i += 4) {\n",
        "                const int tileCol = threadCol * colsPerThread + i;\n",
        "\n",
        "                if (tileCol + 3 < BLOCK_SIZE * 4) {\n",
        "                    const int globalCol = X_tileCol + tileCol;\n",
        "\n",
        "                    if (globalRow < num_samples && globalCol + 3 < input_dim) {\n",
        "                        // Vectorized load with proper alignment\n",
        "                        const float4 vecX = __ldg(reinterpret_cast<const float4*>(&X[globalRow * input_dim + globalCol]));\n",
        "                        X_tile[threadRow * TM + m][tileCol + 0] = vecX.x;\n",
        "                        X_tile[threadRow * TM + m][tileCol + 1] = vecX.y;\n",
        "                        X_tile[threadRow * TM + m][tileCol + 2] = vecX.z;\n",
        "                        X_tile[threadRow * TM + m][tileCol + 3] = vecX.w;\n",
        "                    } else {\n",
        "                        //if total columns are not divisible by 4\n",
        "                        #pragma unroll\n",
        "                        for (int j = 0; j < 4; j++) {\n",
        "                            const int col = globalCol + j;\n",
        "                            X_tile[threadRow * TM + m][tileCol + j] =\n",
        "                                (globalRow < num_samples && col < input_dim) ?\n",
        "                                __ldg(&X[globalRow * input_dim + col]) : 0.0f;\n",
        "                        }\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "\n",
        "        #pragma unroll\n",
        "        for (int k = 0; k < TM; k++) {\n",
        "            #pragma unroll\n",
        "            for (int n = 0; n < TN; n++) {\n",
        "                int loadRow = W_tileRow + threadRow * TM + k;\n",
        "                int loadCol = globalColStart + n;\n",
        "                int smemRow = threadRow * TM + k;\n",
        "                int smemCol = threadCol * TN + n;\n",
        "\n",
        "                if (loadRow < input_dim && loadCol < num_features &&\n",
        "                    smemRow < BLOCK_SIZE * 4 && smemCol < BLOCK_SIZE) {\n",
        "                    W_tile[smemRow][smemCol] = W[loadRow * num_features + loadCol];\n",
        "                } else if (smemRow < BLOCK_SIZE * 4 && smemCol < BLOCK_SIZE) {\n",
        "                    W_tile[smemRow][smemCol] = 0.0f;\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "\n",
        "        __syncthreads();\n",
        "\n",
        "        for (int k = 0; k < BLOCK_SIZE * 4; k++) {\n",
        "            float W_cache[TN];\n",
        "            for (int n = 0; n < TN; n++) {\n",
        "                W_cache[n] = W_tile[k][threadCol * TN + n];\n",
        "            }\n",
        "            // Compute TM x TN partial products\n",
        "            for (int m = 0; m < TM; m++) {\n",
        "                float X_val = X_tile[threadRow * TM + m][k];\n",
        "                for (int n = 0; n < TN; n++) {\n",
        "                    threadResults[m][n] += X_val * W_cache[n];\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // Write results to global memory\n",
        "    // Each thread writes TM x TN output elements\n",
        "    for (int m = 0; m < TM; m++) {\n",
        "        for (int n = 0; n < TN; n++) {\n",
        "            int outputRow = globalRowStart + m;\n",
        "            int outputCol = globalColStart + n;\n",
        "            if (outputRow < num_samples && outputCol < num_features) {\n",
        "                Y[outputRow * num_features + outputCol] = threadResults[m][n];\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "// cuBLAS result: 3.108ms\n",
        "int main() {\n",
        "    int num_samples = 2048, input_dim = 1024, num_features = 5120;\n",
        "    size_t size_X = num_samples * input_dim * sizeof(float);\n",
        "    size_t size_W = input_dim * num_features * sizeof(float);\n",
        "    size_t size_Y = num_samples * num_features * sizeof(float);\n",
        "\n",
        "    float *h_X = (float*)malloc(size_X);\n",
        "    float *h_W = (float*)malloc(size_W);\n",
        "    float *h_W_t = (float*)malloc(size_W);\n",
        "    float *h_Y_gpu_naive = (float*)malloc(size_Y);\n",
        "    float *h_Y_gpu_opt = (float*)malloc(size_Y);\n",
        "    float *h_Y_gpu_shared = (float*)malloc(size_Y);\n",
        "    float *h_Y_gpu_tiled = (float*)malloc(size_Y);\n",
        "    float* h_Y_gpu_vectorized = (float*)malloc(size_Y);\n",
        "\n",
        "    float *h_Y_cublas = (float*)malloc(size_Y);\n",
        "\n",
        "    for (int i = 0; i < num_samples * input_dim; i++) {\n",
        "        h_X[i] = (float)rand() / RAND_MAX;\n",
        "    }\n",
        "    for (int i = 0; i < input_dim * num_features; i++) {\n",
        "        h_W[i] = (float)rand() / RAND_MAX;\n",
        "    }\n",
        "\n",
        "    float *d_X, *d_W, *d_W_t,*d_Y_vec, *d_Y;\n",
        "    cudaMalloc(&d_X, size_X);\n",
        "    cudaMalloc(&d_W, size_W);\n",
        "    cudaMalloc(&d_W_t, size_W);\n",
        "    cudaMalloc(&d_Y, size_Y);\n",
        "    cudaMalloc(&d_Y_vec, size_Y);\n",
        "\n",
        "\n",
        "    cudaMemcpy(d_X, h_X, size_X, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_W, h_W, size_W, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Initialize cuBLAS\n",
        "    cublasHandle_t cublas_handle;\n",
        "    cublasCreate(&cublas_handle);\n",
        "\n",
        "    dim3 threads(BLOCK_SIZE, BLOCK_SIZE);\n",
        "    dim3 blocks((num_features + BLOCK_SIZE - 1) / BLOCK_SIZE,\n",
        "                (num_samples + BLOCK_SIZE - 1) / BLOCK_SIZE);\n",
        "\n",
        "    // Thread-tiled kernel configuration\n",
        "    dim3 threadsPerBlock_tiled(BLOCK_SIZE/TN, BLOCK_SIZE/TM);\n",
        "    dim3 numBlocks_tiled((num_features + BLOCK_SIZE - 1) / BLOCK_SIZE,\n",
        "                         (num_samples + BLOCK_SIZE - 1) / BLOCK_SIZE);\n",
        "\n",
        "\n",
        "    // Set up grid and block dimensions for vectorized kernel\n",
        "    dim3 threadsPerBlock_vectorized(BLOCK_SIZE / TN, BLOCK_SIZE / TM);\n",
        "    dim3 numBlocks_vectorized((num_features + BLOCK_SIZE - 1) / BLOCK_SIZE,\n",
        "                             (num_samples + BLOCK_SIZE - 1) / BLOCK_SIZE);\n",
        "\n",
        "    // Transpose W on GPU\n",
        "    dim3 threads_t(BLOCK_SIZE, BLOCK_SIZE);\n",
        "    dim3 blocks_t((num_features + BLOCK_SIZE - 1) / BLOCK_SIZE,\n",
        "                  (input_dim + BLOCK_SIZE - 1) / BLOCK_SIZE);\n",
        "\n",
        "    transpose_kernel<<<blocks_t, threads_t>>>(d_W, d_W_t, input_dim, num_features);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "\n",
        "    // Warmup kernels\n",
        "    matmul_kernel<<<blocks, threads>>>(d_X, d_W, d_Y, num_samples, input_dim, num_features);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    matmul_kernel_optimized<<<blocks, threads>>>(d_X, d_W_t, d_Y, num_samples, input_dim, num_features);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    matmul_kernel_shared<<<blocks, threads>>>(d_X, d_W, d_Y, num_samples, input_dim, num_features);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    matmul_kernel_threadreduce<<<numBlocks_tiled, threadsPerBlock_tiled>>>(d_X, d_W, d_Y, num_samples, input_dim, num_features);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    matmul_vectorized<<<numBlocks_vectorized, threadsPerBlock_vectorized>>>(\n",
        "        d_X, d_W, d_Y, num_samples, input_dim, num_features);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    // cuBLAS warmup\n",
        "    const float alpha = 1.0f, beta = 0.0f;\n",
        "    cublasSgemm(cublas_handle, CUBLAS_OP_N, CUBLAS_OP_N,\n",
        "                num_features, num_samples, input_dim,\n",
        "                &alpha, d_W, num_features, d_X, input_dim, &beta, d_Y, num_features);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    // Naive GPU kernel timing\n",
        "    float total_naive_gpu_time = 0.0f;\n",
        "    for (int i = 0; i < RUNS; ++i) {\n",
        "        cudaEventRecord(start);\n",
        "        matmul_kernel<<<blocks, threads>>>(d_X, d_W, d_Y, num_samples, input_dim, num_features);\n",
        "        cudaEventRecord(stop);\n",
        "        cudaEventSynchronize(stop);\n",
        "        float this_time;\n",
        "        cudaEventElapsedTime(&this_time, start, stop);\n",
        "        total_naive_gpu_time += this_time;\n",
        "    }\n",
        "    float avg_naive_gpu_time = total_naive_gpu_time / RUNS;\n",
        "    cudaMemcpy(h_Y_gpu_naive, d_Y, size_Y, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Optimized GPU kernel timing\n",
        "    float total_opt_gpu_time = 0.0f;\n",
        "    for (int i = 0; i < RUNS; ++i) {\n",
        "        cudaEventRecord(start);\n",
        "        matmul_kernel_optimized<<<blocks, threads>>>(d_X, d_W_t, d_Y, num_samples, input_dim, num_features);\n",
        "        cudaEventRecord(stop);\n",
        "        cudaEventSynchronize(stop);\n",
        "        float this_time;\n",
        "        cudaEventElapsedTime(&this_time, start, stop);\n",
        "        total_opt_gpu_time += this_time;\n",
        "    }\n",
        "    float avg_opt_gpu_time = total_opt_gpu_time / RUNS;\n",
        "    cudaMemcpy(h_Y_gpu_opt, d_Y, size_Y, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Shared memory GPU kernel timing\n",
        "    float total_shared_time = 0.0f;\n",
        "    for (int i = 0; i < RUNS; ++i) {\n",
        "        cudaEventRecord(start);\n",
        "        matmul_kernel_shared<<<blocks, threads>>>(d_X, d_W, d_Y,\n",
        "                                                  num_samples, input_dim, num_features);\n",
        "        cudaEventRecord(stop);\n",
        "        cudaEventSynchronize(stop);\n",
        "        float this_time;\n",
        "        cudaEventElapsedTime(&this_time, start, stop);\n",
        "        total_shared_time += this_time;\n",
        "    }\n",
        "    float avg_shared_time = total_shared_time / RUNS;\n",
        "    cudaMemcpy(h_Y_gpu_shared, d_Y, size_Y, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Thread-tiled GPU kernel timing\n",
        "    float total_tiled_time = 0.0f;\n",
        "    for (int i = 0; i < RUNS; ++i) {\n",
        "        cudaEventRecord(start);\n",
        "        matmul_kernel_threadreduce<<<numBlocks_tiled, threadsPerBlock_tiled>>>(d_X, d_W, d_Y,\n",
        "                                                                       num_samples, input_dim, num_features);\n",
        "        cudaEventRecord(stop);\n",
        "        cudaEventSynchronize(stop);\n",
        "        float this_time;\n",
        "        cudaEventElapsedTime(&this_time, start, stop);\n",
        "        total_tiled_time += this_time;\n",
        "    }\n",
        "    float avg_tiled_time = total_tiled_time / RUNS;\n",
        "    cudaMemcpy(h_Y_gpu_tiled, d_Y, size_Y, cudaMemcpyDeviceToHost);\n",
        "\n",
        "\n",
        "\n",
        "    // Timing runs for vectorized kernel\n",
        "    float total_vectorized_time = 0.0f;\n",
        "    for (int i = 0; i < RUNS; ++i) {\n",
        "        cudaEventRecord(start);\n",
        "        matmul_vectorized<<<numBlocks_vectorized, threadsPerBlock_vectorized>>>(\n",
        "            d_X, d_W, d_Y_vec, num_samples, input_dim, num_features);\n",
        "        cudaEventRecord(stop);\n",
        "        cudaEventSynchronize(stop);\n",
        "        float this_time;\n",
        "        cudaEventElapsedTime(&this_time, start, stop);\n",
        "        total_vectorized_time += this_time;\n",
        "    }\n",
        "    float avg_vectorized_time = total_vectorized_time / RUNS;\n",
        "    cudaMemcpy(h_Y_gpu_vectorized, d_Y, size_Y, cudaMemcpyDeviceToHost);\n",
        "\n",
        "\n",
        "    float total_cublas_time = 0.0f;\n",
        "    for (int i = 0; i < RUNS; ++i) {\n",
        "        cudaEventRecord(start);\n",
        "        cublasSgemm(cublas_handle, CUBLAS_OP_N, CUBLAS_OP_N,\n",
        "                    num_features, num_samples, input_dim,\n",
        "                    &alpha, d_W, num_features, d_X, input_dim, &beta, d_Y, num_features);\n",
        "        cudaEventRecord(stop);\n",
        "        cudaEventSynchronize(stop);\n",
        "        float this_time;\n",
        "        cudaEventElapsedTime(&this_time, start, stop);\n",
        "        total_cublas_time += this_time;\n",
        "    }\n",
        "    float avg_cublas_time = total_cublas_time / RUNS;\n",
        "    cudaMemcpy(h_Y_cublas, d_Y, size_Y, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Compare results for correctness\n",
        "    float max_diff_naive_opt = 0.0f;\n",
        "    float max_diff_naive_shared = 0.0f;\n",
        "    float max_diff_naive_tiled = 0.0f;\n",
        "    float max_diff_naive_cublas = 0.0f;\n",
        "    float max_diff_naive_vectorized = 0.0f;\n",
        "\n",
        "    for (int i = 0; i < num_samples * num_features; ++i) {\n",
        "        float diff_opt = fabs(h_Y_gpu_naive[i] - h_Y_gpu_opt[i]);\n",
        "        float diff_shared = fabs(h_Y_gpu_naive[i] - h_Y_gpu_shared[i]);\n",
        "        float diff_tiled = fabs(h_Y_gpu_naive[i] - h_Y_gpu_tiled[i]);\n",
        "        float diff_cublas = fabs(h_Y_gpu_naive[i] - h_Y_cublas[i]);\n",
        "        float diff_vectorized = fabs(h_Y_gpu_naive[i] - h_Y_gpu_vectorized[i]);\n",
        "\n",
        "        if (diff_opt > max_diff_naive_opt) max_diff_naive_opt = diff_opt;\n",
        "        if (diff_shared > max_diff_naive_shared) max_diff_naive_shared = diff_shared;\n",
        "        if (diff_tiled > max_diff_naive_tiled) max_diff_naive_tiled = diff_tiled;\n",
        "        if (diff_cublas > max_diff_naive_cublas) max_diff_naive_cublas = diff_cublas;\n",
        "        if (diff_vectorized > max_diff_naive_vectorized) max_diff_naive_vectorized = diff_vectorized;\n",
        "    }\n",
        "\n",
        "    // Print results\n",
        "    printf(\"=== PERFORMANCE RESULTS ===\");\n",
        "    printf(\"--- Naive GPU Kernel ---\");\n",
        "    printf(\"Avg time: %.3f ms\", avg_naive_gpu_time);\n",
        "    puts(\"\");\n",
        "    printf(\"--- Optimized GPU Kernel ---\");\n",
        "    printf(\"Avg time: %.3f ms\", avg_opt_gpu_time);\n",
        "    printf(\"Naive vs Optimized: %.2fx\", avg_opt_gpu_time / avg_naive_gpu_time);\n",
        "    puts(\"\");\n",
        "    printf(\"--- Shared Memory GPU Kernel ---\");\n",
        "    printf(\"Avg time: %.3f ms\", avg_shared_time);\n",
        "    printf(\"Shared vs Naive: %.2fx\", avg_naive_gpu_time / avg_shared_time);\n",
        "    printf(\"Shared vs Optimized: %.2fx\", avg_opt_gpu_time / avg_shared_time);\n",
        "    puts(\"\");\n",
        "    printf(\"--- Thread-Tiled GPU Kernel (TM=%d, TN=%d) ---\", TM, TN);\n",
        "    printf(\"Avg time: %.3f ms\", avg_tiled_time);\n",
        "    printf(\"Threads per block: %dx%d = %d (vs %dx%d = %d)\",\n",
        "           threadsPerBlock_tiled.x, threadsPerBlock_tiled.y,\n",
        "           threadsPerBlock_tiled.x * threadsPerBlock_tiled.y,\n",
        "           threads.x, threads.y, threads.x * threads.y);\n",
        "    printf(\"Tiled vs Naive: %.2fx\", avg_naive_gpu_time / avg_tiled_time);\n",
        "    printf(\"Tiled vs Shared: %.2fx\", avg_shared_time / avg_tiled_time);\n",
        "    puts(\"\");\n",
        "\n",
        "    printf(\"--- Vectorized Shared Memory GPU Kernel ---\");\n",
        "    printf(\"Avg time: %.3f ms\", avg_vectorized_time);\n",
        "    printf(\"Vectorized vs Naive: %.2fx\", avg_naive_gpu_time / avg_vectorized_time);\n",
        "    printf(\"Vectorized vs Shared: %.2fx\", avg_shared_time / avg_vectorized_time);\n",
        "    printf(\"Vectorized vs Thread-Tiled: %.2fx\", avg_tiled_time / avg_vectorized_time);\n",
        "    printf(\"Vectorized vs cuBLAS: %.2fx\", avg_cublas_time / avg_vectorized_time);\n",
        "    puts(\"\");\n",
        "\n",
        "    printf(\"--- cuBLAS ---\");\n",
        "    printf(\"Avg time: %.3f ms\", avg_cublas_time);\n",
        "    printf(\"cuBLAS vs Naive: %.2fx\", avg_naive_gpu_time / avg_cublas_time);\n",
        "    printf(\"cuBLAS vs Optimized: %.2fx\", avg_opt_gpu_time / avg_cublas_time);\n",
        "    printf(\"cuBLAS vs Shared: %.2fx\", avg_shared_time / avg_cublas_time);\n",
        "    puts(\"\");\n",
        "\n",
        "    printf(\"=== CORRECTNESS CHECK ===\");\n",
        "    printf(\"Max abs diff (Naive vs Optimized): %e\", max_diff_naive_opt);\n",
        "    printf(\"Max abs diff (Naive vs Shared): %e\", max_diff_naive_shared);\n",
        "    printf(\"Max abs diff (Naive vs Thread-Tiled): %e\", max_diff_naive_tiled);\n",
        "    printf(\"Max abs diff (Naive vs Vectorized): %e\", max_diff_naive_vectorized);\n",
        "    printf(\"Max abs diff (Naive vs cuBLAS): %e\", max_diff_naive_cublas);\n",
        "\n",
        "    // Cleanup\n",
        "    cublasDestroy(cublas_handle);\n",
        "    cudaFree(d_X);\n",
        "    cudaFree(d_W);\n",
        "    cudaFree(d_W_t);\n",
        "    cudaFree(d_Y);\n",
        "    free(h_X);\n",
        "    free(h_W);\n",
        "    free(h_W_t);\n",
        "    free(h_Y_gpu_naive);\n",
        "    free(h_Y_gpu_opt);\n",
        "    free(h_Y_gpu_shared);\n",
        "    free(h_Y_cublas);\n",
        "    cudaEventDestroy(start);\n",
        "    cudaEventDestroy(stop);\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "zTi0zFnJ9fvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"matmult.cu\", \"w\") as f:\n",
        "    f.write(code)"
      ],
      "metadata": {
        "id": "Bbm-oe2O9lSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_75 matmult.cu -lcublas -o matmult\n",
        "!./matmult"
      ],
      "metadata": {
        "id": "-1LObqcA9lYZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}